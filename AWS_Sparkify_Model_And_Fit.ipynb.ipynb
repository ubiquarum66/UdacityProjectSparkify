{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3507359d1e3d4f9d91c97d4abb158214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1590345673634_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-40-155.us-east-2.compute.internal:20888/proxy/application_1590345673634_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-42-47.us-east-2.compute.internal:8042/node/containerlogs/container_1590345673634_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172439b807db41a1a7c844105a67c22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No module named 'pandas'\n",
      "Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'pandas'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import further libraries\n",
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf,count,when,isnan,regexp_replace,countDistinct,month,from_unixtime,to_timestamp,lead,datediff,mean\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.feature import MinMaxScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299fcf7279c9467ab3d9af2f205f59b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder.appName(\"SparkifyThomas\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5eb4c72a1540449d47bbf655da890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3d73e63d6f4a2ba85a62832abb115a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "user_log = spark.read.json(event_data)\n",
    "user_log.persist()\n",
    "user_log = user_log.filter(user_log[\"userId\"] != \"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Churn dataset as in 'data wrangling with DataFrame'\n",
    "# In[11]:\n",
    "flag_downgrade_event = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\n",
    "user_log = user_log.withColumn(\"downgraded\", flag_downgrade_event(\"page\"))\n",
    "# now we have for each timestamp sorted user a delta peak where the downgrade took place\n",
    "from pyspark.sql import Window\n",
    "windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "user_log = user_log.withColumn(\"churn\", Fsum(\"downgraded\").over(windowval))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time = udf(lambda x: pd.Timestamp(x / 1000.0,unit='s').strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "user_log = user_log.withColumn(\"time\", get_time(user_log.ts)).withColumn('registration_time',get_time(user_log.registration))\n",
    "user_log = user_log.withColumn('date', from_unixtime(col('ts')/1000).cast(DateType()))\n",
    "user_log = user_log.withColumn('month', month(\"time\").alias('month'))\n",
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "churningusers = user_log.groupBy(\"userId\").max(\"churn\").withColumnRenamed(\"max(churn)\", \"churn\")\n",
    "churningusers.select([\"userId\", \"churn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Recoding of the categorical variables....\n",
    "transformchurn  = udf(lambda x: 1 if x > 0  else 0, IntegerType()) # GaussBost Binary!\n",
    "transformgender = udf(lambda x: 1 if x == \"F\" else 0, IntegerType())\n",
    "transformlevel =  udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "user_temp1 = user_log.withColumn(\"gender\", transformgender(\"gender\"))\n",
    "user_temp2 = user_log.withColumn(\"level\",  transformlevel (\"level\"))\n",
    "user_temp3 = user_log.withColumn(\"churngbt\",  transformlevel (\"churn\"))\n",
    "user_temp1 = user_temp1.groupby('userId').agg({\"gender\": \"max\"}).withColumnRenamed(\"max(gender)\", \"gender\")\n",
    "user_temp2 = user_temp2.groupby('userId').agg({\"level\": \"max\"}).withColumnRenamed(\"max(level)\", \"level\")\n",
    "user_temp3 = user_temp3.groupby('userId').agg({\"churngbt\": \"max\"}).withColumnRenamed(\"max(churngbt)\", \"churngbt\")\n",
    "churningusers=churningusers.join(user_temp1, ['userId'])\n",
    "churningusers=churningusers.join(user_temp2, ['userId'])\n",
    "churningusers=churningusers.join(user_temp3, ['userId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, join all of them together to the feature set...\n",
    "# \n",
    "# In[100]:\n",
    "songcount = user_log.groupby(\"userId\").agg(countDistinct(\"song\"))\n",
    "artistdiversity = user_log.groupby(\"userId\").agg(countDistinct(\"artist\"))\n",
    "sumoflength=user_log.groupby(\"userId\").agg({\"length\" : \"sum\"})\n",
    "visitfrequency = user_log.groupby(\"userId\").count()\n",
    "songcount = songcount.withColumnRenamed(\"count(DISTINCT song)\", \"songcount\")\n",
    "artistdiversity = artistdiversity.withColumnRenamed(\"count(DISTINCT artist)\", \"artistdiversity\")\n",
    "sumoflength=sumoflength.withColumnRenamed(\"sum(length)\",\"sumoflength\")\n",
    "visitfrequency = visitfrequency.withColumnRenamed(\"count\", \"visitfrequency\")\n",
    "first_interaction =  user_log.groupBy('userId').agg(min('ts').alias('first_interaction'))\n",
    "last_interaction =  user_log.groupBy('userId').agg(max('ts').alias('last_interaction'))\n",
    "mean_interaction =  user_log.groupBy('userId').agg(mean('ts').alias('mean_interaction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again the userid with the known churning as a test\n",
    "churningusers  = churningusers.join(artistdiversity,[\"userId\"])\n",
    "churningusers  = churningusers.join(visitfrequency,[\"userId\"])\n",
    "churningusers  = churningusers.join(songcount,[\"userId\"])\n",
    "churningusers  = churningusers.join(sumoflength,[\"userId\"])\n",
    "churningusers  = churningusers.join(first_interaction,[\"userId\"])\n",
    "churningusers  = churningusers.join(last_interaction,[\"userId\"])\n",
    "churningusers  = churningusers.join(mean_interaction,[\"userId\"])\n",
    "churningusers = churningusers.withColumn('lastedinteraction', churningusers['last_interaction']-churningusers['first_interaction'])\n",
    "churningusers = churningusers.withColumn('lastedinteraction', churningusers['lastedinteraction']/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "churningusers.printSchema()\n",
    "churningusers.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in churningusers.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "churningusers= churningusers.drop(\"userId\")\n",
    "getallfeatures = [mycol for mycol in churningusers.columns if mycol not in ['churn','churngbt']]\n",
    "print(getallfeatures)\n",
    "getallfeaturesall = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount', 'sumoflength', 'first_interaction', 'last_interaction', 'mean_interaction', 'lastedinteraction']\n",
    "\n",
    "getallfeatures1 = ['gender', 'level']\n",
    "getallfeatures2 = ['gender', 'level', 'artistdiversity' ]\n",
    "getallfeatures3 = ['gender', 'level', 'artistdiversity', 'visitfrequency']\n",
    "getallfeatures4 = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount']\n",
    "getallfeatures5 = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount', 'sumoflength']\n",
    "getallfeatures = getallfeatures1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, userid is no longer needed, as the model is lter on served by user data, without knowlegde of origin\n",
    "\n",
    "churningusers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test = churningusers.randomSplit([0.7, 0.3], seed=42)\n",
    "train = train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelbuilder(myclassifier, myparam, mynumfold,labelcolin):\n",
    "    \"\"\"\n",
    "    Try to setup a pipeline with given Clasifier\n",
    "    Input:\n",
    "    myclassifier - one type of vailable classifier classes\n",
    "    param - built param grid for  model opt search\n",
    "    Output :\n",
    "    mymodel - ML pipeline model as transformer...\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=getallfeatures, outputCol=\"features\")\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scfeatures\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, myclassifier])\n",
    "    mymodel = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=myparam,\n",
    "        evaluator = \\\n",
    "          MulticlassClassificationEvaluator( \\\n",
    "          labelCol=labelcolin, metricName='f1'),\n",
    "        numFolds=mynumfold,\n",
    "    )\n",
    "    return mymodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndf = RandomForestClassifier(featuresCol=\"scfeatures\", labelCol=\"churn\")\n",
    "rndf_param = ParamGridBuilder().build()\n",
    "rndf_model = modelbuilder(rndf, rndf_param,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndf_fitted_model = rndf_model.fit(train)\n",
    "rndf_pred = rndf_fitted_model.transform(test)\n",
    "\n",
    "rndfscore = evaluator.evaluate(rndf_pred, {evaluator.metricName: \"f1\"})\n",
    "print(\"f1 using Random Forest: {}\".format(rndfscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churngbt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussboost =GBTClassifier(featuresCol=\"scfeatures\", labelCol=\"churngbt\")\n",
    "gaussboost_param = ParamGridBuilder().build()\n",
    "gaussboost_model = modelbuilder(gaussboost, gaussboost_param,2,'churngbt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussboost_f1score = evaluator.evaluate(gaussboost_pred, {evaluator.metricName: \"f1\"})\n",
    "print(\"f1 using Gradient Boosting: {}\".format(gaussboost_f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
