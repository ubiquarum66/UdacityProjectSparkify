{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f444c0509b4645549909b8d1b2b0de47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327a0f3baf64441d83d29cc3866975ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No module named 'pandas'\n",
      "Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'pandas'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import further libraries\n",
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf,count,when,isnan,regexp_replace,countDistinct,month,from_unixtime,to_timestamp,lead,datediff,mean\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.feature import MinMaxScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b5c55d817e4b018c7a175d82c71d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder.appName(\"SparkifyThomas\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0080ffd2698a486885498796c4304aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "user_log = spark.read.json(event_data)\n",
    "user_log.persist()\n",
    "user_log = user_log.filter(user_log[\"userId\"] != \"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c41c0920ed84c25955d353a181009d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ### Churn dataset as in 'data wrangling with DataFrame'\n",
    "# In[11]:\n",
    "flag_downgrade_event = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\n",
    "user_log = user_log.withColumn(\"downgraded\", flag_downgrade_event(\"page\"))\n",
    "# now we have for each timestamp sorted user a delta peak where the downgrade took place\n",
    "from pyspark.sql import Window\n",
    "windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "user_log = user_log.withColumn(\"churn\", Fsum(\"downgraded\").over(windowval))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39e7398391e457689bc3b1c8132e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o154.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 195, ip-172-31-42-47.us-east-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1251, in head\n",
      "    rs = self.head(1)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1253, in head\n",
      "    return self.take(n)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 572, in take\n",
      "    return self.limit(num).collect()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 534, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o154.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 195, ip-172-31-42-47.us-east-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0005/container_1590345673634_0005_01_000002/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_time = udf(lambda x: pd.Timestamp(x / 1000.0,unit='s').strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "user_log = user_log.withColumn(\"time\", get_time(user_log.ts)).withColumn('registration_time',get_time(user_log.registration))\n",
    "user_log = user_log.withColumn('date', from_unixtime(col('ts')/1000).cast(DateType()))\n",
    "user_log = user_log.withColumn('month', month(\"time\").alias('month'))\n",
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc09bb2b43a24d1592d446220ad4e5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: string, churn: bigint]"
     ]
    }
   ],
   "source": [
    "\n",
    "churningusers = user_log.groupBy(\"userId\").max(\"churn\").withColumnRenamed(\"max(churn)\", \"churn\")\n",
    "churningusers.select([\"userId\", \"churn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf747a173e84885b7811beab4db91c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Recoding of the categorical variables....\n",
    "transformchurn  = udf(lambda x: 1 if x > 0  else 0, IntegerType()) # GaussBost Binary!\n",
    "transformgender = udf(lambda x: 1 if x == \"F\" else 0, IntegerType())\n",
    "transformlevel =  udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "user_temp1 = user_log.withColumn(\"gender\", transformgender(\"gender\"))\n",
    "user_temp2 = user_log.withColumn(\"level\",  transformlevel (\"level\"))\n",
    "user_temp3 = user_log.withColumn(\"churngbt\",  transformlevel (\"churn\"))\n",
    "user_temp1 = user_temp1.groupby('userId').agg({\"gender\": \"max\"}).withColumnRenamed(\"max(gender)\", \"gender\")\n",
    "user_temp2 = user_temp2.groupby('userId').agg({\"level\": \"max\"}).withColumnRenamed(\"max(level)\", \"level\")\n",
    "user_temp3 = user_temp3.groupby('userId').agg({\"churngbt\": \"max\"}).withColumnRenamed(\"max(churngbt)\", \"churngbt\")\n",
    "churningusers=churningusers.join(user_temp1, ['userId'])\n",
    "churningusers=churningusers.join(user_temp2, ['userId'])\n",
    "churningusers=churningusers.join(user_temp3, ['userId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94fb94aae1f4b12a08131d30a0d3096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, join all of them together to the feature set...\n",
    "# \n",
    "# In[100]:\n",
    "songcount = user_log.groupby(\"userId\").agg(countDistinct(\"song\"))\n",
    "artistdiversity = user_log.groupby(\"userId\").agg(countDistinct(\"artist\"))\n",
    "sumoflength=user_log.groupby(\"userId\").agg({\"length\" : \"sum\"})\n",
    "visitfrequency = user_log.groupby(\"userId\").count()\n",
    "songcount = songcount.withColumnRenamed(\"count(DISTINCT song)\", \"songcount\")\n",
    "artistdiversity = artistdiversity.withColumnRenamed(\"count(DISTINCT artist)\", \"artistdiversity\")\n",
    "sumoflength=sumoflength.withColumnRenamed(\"sum(length)\",\"sumoflength\")\n",
    "visitfrequency = visitfrequency.withColumnRenamed(\"count\", \"visitfrequency\")\n",
    "first_interaction =  user_log.groupBy('userId').agg(min('ts').alias('first_interaction'))\n",
    "last_interaction =  user_log.groupBy('userId').agg(max('ts').alias('last_interaction'))\n",
    "mean_interaction =  user_log.groupBy('userId').agg(mean('ts').alias('mean_interaction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba4b8e81fd34cbcafb7c82c81a0b53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# again the userid with the known churning as a test\n",
    "churningusers  = churningusers.join(artistdiversity,[\"userId\"])\n",
    "churningusers  = churningusers.join(visitfrequency,[\"userId\"])\n",
    "churningusers  = churningusers.join(songcount,[\"userId\"])\n",
    "churningusers  = churningusers.join(sumoflength,[\"userId\"])\n",
    "churningusers  = churningusers.join(first_interaction,[\"userId\"])\n",
    "churningusers  = churningusers.join(last_interaction,[\"userId\"])\n",
    "churningusers  = churningusers.join(mean_interaction,[\"userId\"])\n",
    "churningusers = churningusers.withColumn('lastedinteraction', churningusers['last_interaction']-churningusers['first_interaction'])\n",
    "churningusers = churningusers.withColumn('lastedinteraction', churningusers['lastedinteraction']/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37ad622505a4481982e4753e7888921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- churn: long (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- churngbt: integer (nullable = true)\n",
      " |-- artistdiversity: long (nullable = false)\n",
      " |-- visitfrequency: long (nullable = false)\n",
      " |-- songcount: long (nullable = false)\n",
      " |-- sumoflength: double (nullable = true)\n",
      " |-- first_interaction: long (nullable = true)\n",
      " |-- last_interaction: long (nullable = true)\n",
      " |-- mean_interaction: double (nullable = true)\n",
      " |-- lastedinteraction: double (nullable = true)\n",
      "\n",
      "[Row(userId=0, churn=0, gender=0, level=0, churngbt=0, artistdiversity=0, visitfrequency=0, songcount=0, sumoflength=17, first_interaction=0, last_interaction=0, mean_interaction=0, lastedinteraction=0)]"
     ]
    }
   ],
   "source": [
    "\n",
    "churningusers.printSchema()\n",
    "churningusers.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in churningusers.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9667e52486b04cdbb349ba66ea180679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount', 'sumoflength', 'first_interaction', 'last_interaction', 'mean_interaction', 'lastedinteraction']"
     ]
    }
   ],
   "source": [
    "\n",
    "churningusers= churningusers.drop(\"userId\")\n",
    "getallfeatures = [mycol for mycol in churningusers.columns if mycol not in ['churn','churngbt']]\n",
    "print(getallfeatures)\n",
    "getallfeaturesall = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount', 'sumoflength', 'first_interaction', 'last_interaction', 'mean_interaction', 'lastedinteraction']\n",
    "\n",
    "getallfeatures1 = ['gender', 'level']\n",
    "getallfeatures2 = ['gender', 'level', 'artistdiversity' ]\n",
    "getallfeatures3 = ['gender', 'level', 'artistdiversity', 'visitfrequency']\n",
    "getallfeatures4 = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount']\n",
    "getallfeatures5 = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount', 'sumoflength']\n",
    "getallfeatures = getallfeatures1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0a05c122864302969df8ec5c661f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- churn: long (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- churngbt: integer (nullable = true)\n",
      " |-- artistdiversity: long (nullable = false)\n",
      " |-- visitfrequency: long (nullable = false)\n",
      " |-- songcount: long (nullable = false)\n",
      " |-- sumoflength: double (nullable = true)\n",
      " |-- first_interaction: long (nullable = true)\n",
      " |-- last_interaction: long (nullable = true)\n",
      " |-- mean_interaction: double (nullable = true)\n",
      " |-- lastedinteraction: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "#here, userid is no longer needed, as the model is lter on served by user data, without knowlegde of origin\n",
    "\n",
    "churningusers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acaa30e4fec54c209e97474290aa36a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train, test = churningusers.randomSplit([0.7, 0.3], seed=42)\n",
    "train = train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dd98beaee949b98bd3a5d648b18f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def modelbuilder(myclassifier, myparam, mynumfold,labelcolin):\n",
    "    \"\"\"\n",
    "    Try to setup a pipeline with given Clasifier\n",
    "    Input:\n",
    "    myclassifier - one type of vailable classifier classes\n",
    "    param - built param grid for  model opt search\n",
    "    Output :\n",
    "    mymodel - ML pipeline model as transformer...\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=getallfeatures, outputCol=\"features\")\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scfeatures\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, myclassifier])\n",
    "    mymodel = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=myparam,\n",
    "        evaluator = \\\n",
    "          MulticlassClassificationEvaluator( \\\n",
    "          labelCol=labelcolin, metricName='f1'),\n",
    "        numFolds=mynumfold,\n",
    "    )\n",
    "    return mymodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d16929e083748e9b0b0638fb78e3278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d55f9711e14527902cffcd37770f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rndf = RandomForestClassifier(featuresCol=\"scfeatures\", labelCol=\"churn\")\n",
    "rndf_param = ParamGridBuilder().build()\n",
    "rndf_model = modelbuilder(rndf, rndf_param,2,'churn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b9f1e0e3474d14a61eee4f5f84ebf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 using Random Forest: 0.6651134017337476"
     ]
    }
   ],
   "source": [
    "rndf_fitted_model = rndf_model.fit(train)\n",
    "rndf_pred = rndf_fitted_model.transform(test)\n",
    "\n",
    "rndfscore = evaluator.evaluate(rndf_pred, {evaluator.metricName: \"f1\"})\n",
    "print(\"f1 using Random Forest: {}\".format(rndfscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ad5a54f45a4b9b9aa39b34032d2bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churngbt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a252ff94244bd8a6cebea39bb08ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gaussboost =GBTClassifier(featuresCol=\"scfeatures\", labelCol=\"churngbt\")\n",
    "gaussboost_param = ParamGridBuilder().build()\n",
    "gaussboost_model = modelbuilder(gaussboost, gaussboost_param,2,'churngbt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab44d093fcb45e086e3cabc95f323c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n",
      "    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\n",
      "KeyError: 3152\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(prediction=0.0)]"
     ]
    }
   ],
   "source": [
    "gaussboost_fitted_model = gaussboost_model.fit(train)\n",
    "gaussboost_pred = gaussboost_fitted_model.transform(test)\n",
    "gaussboost_pred.select(\"prediction\").dropDuplicates().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029eccdcb26441538d1515df3bc2701a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 using Gradient Boosting: 1.0"
     ]
    }
   ],
   "source": [
    "gaussboost_f1score = evaluator.evaluate(gaussboost_pred, {evaluator.metricName: \"f1\"})\n",
    "print(\"f1 using Gradient Boosting: {}\".format(gaussboost_f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
