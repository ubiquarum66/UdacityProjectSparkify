{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f50b7c92754be48af96d1c72a69236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ffcd9e6851446180656fa059f33d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No module named 'pandas'\n",
      "Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'pandas'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import further libraries\n",
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf,count,when,isnan,regexp_replace,countDistinct,month,from_unixtime,to_timestamp,lead,datediff,mean\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.feature import MinMaxScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c146cf5f9a734691b9ea2f7415a399dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder.appName(\"SparkifyThomas\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a1465e32e647aaa819510cd8d9d27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "user_log = spark.read.json(event_data)\n",
    "user_log.persist()\n",
    "user_log = user_log.filter(user_log[\"userId\"] != \"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b461d6f4d724ffe9842a3c49bc118e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ### Churn dataset as in 'data wrangling with DataFrame'\n",
    "# In[11]:\n",
    "flag_downgrade_event = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\n",
    "user_log = user_log.withColumn(\"downgraded\", flag_downgrade_event(\"page\"))\n",
    "# now we have for each timestamp sorted user a delta peak where the downgrade took place\n",
    "from pyspark.sql import Window\n",
    "windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "user_log = user_log.withColumn(\"churn\", Fsum(\"downgraded\").over(windowval))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199b346cb47f42058eeb93401968edc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o1730.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 542.0 failed 4 times, most recent failure: Lost task 0.3 in stage 542.0 (TID 14390, ip-172-31-42-47.us-east-2.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1251, in head\n",
      "    rs = self.head(1)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1253, in head\n",
      "    return self.take(n)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 572, in take\n",
      "    return self.limit(num).collect()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 534, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1730.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 542.0 failed 4 times, most recent failure: Lost task 0.3 in stage 542.0 (TID 14390, ip-172-31-42-47.us-east-2.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1590345673634_0003/container_1590345673634_0003_01_000004/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_time = udf(lambda x: pd.Timestamp(x / 1000.0,unit='s').strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "user_log = user_log.withColumn(\"time\", get_time(user_log.ts)).withColumn('registration_time',get_time(user_log.registration))\n",
    "user_log = user_log.withColumn('date', from_unixtime(col('ts')/1000).cast(DateType()))\n",
    "user_log = user_log.withColumn('month', month(\"time\").alias('month'))\n",
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbdf5944d4e47fa9dc05dbe67d5015d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: string, churn: bigint]"
     ]
    }
   ],
   "source": [
    "\n",
    "churningusers = user_log.groupBy(\"userId\").max(\"churn\").withColumnRenamed(\"max(churn)\", \"churn\")\n",
    "churningusers.select([\"userId\", \"churn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b4b430040a4ea7abbad1ad1c426d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Recoding of the categorical variables....\n",
    "transformgender = udf(lambda x: 1 if x == \"F\" else 0, IntegerType())\n",
    "transformlevel =  udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "user_temp1 = user_log.withColumn(\"gender\", transformgender(\"gender\"))\n",
    "user_temp2 = user_log.withColumn(\"level\",  transformlevel (\"level\"))\n",
    "user_temp1 = user_temp1.groupby('userId').agg({\"gender\": \"max\"}).withColumnRenamed(\"max(gender)\", \"gender\")\n",
    "user_temp2 = user_temp2.groupby('userId').agg({\"level\": \"max\"}).withColumnRenamed(\"max(level)\", \"level\")\n",
    "churningusers=churningusers.join(user_temp1, ['userId'])\n",
    "churningusers=churningusers.join(user_temp2, ['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1ac5fd972b41aba4cc66d344e09ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, join all of them together to the feature set...\n",
    "# \n",
    "# In[100]:\n",
    "songcount = user_log.groupby(\"userId\").agg(countDistinct(\"song\"))\n",
    "artistdiversity = user_log.groupby(\"userId\").agg(countDistinct(\"artist\"))\n",
    "sumoflength=user_log.groupby(\"userId\").agg({\"length\" : \"sum\"})\n",
    "visitfrequency = user_log.groupby(\"userId\").count()\n",
    "songcount = songcount.withColumnRenamed(\"count(DISTINCT song)\", \"songcount\")\n",
    "artistdiversity = artistdiversity.withColumnRenamed(\"count(DISTINCT artist)\", \"artistdiversity\")\n",
    "sumoflength=sumoflength.withColumnRenamed(\"sum(length)\",\"sumoflength\")\n",
    "visitfrequency = visitfrequency.withColumnRenamed(\"count\", \"visitfrequency\")\n",
    "first_interaction =  user_log.groupBy('userId').agg(min('ts').alias('first_interaction'))\n",
    "last_interaction =  user_log.groupBy('userId').agg(max('ts').alias('last_interaction'))\n",
    "mean_interaction =  user_log.groupBy('userId').agg(mean('ts').alias('mean_interaction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e245714341f64ccf90f990c10a1b4b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# again the userid with the known churning as a test\n",
    "churningusers  = churningusers.join(artistdiversity,[\"userId\"])\n",
    "churningusers  = churningusers.join(visitfrequency,[\"userId\"])\n",
    "churningusers  = churningusers.join(songcount,[\"userId\"])\n",
    "churningusers  = churningusers.join(sumoflength,[\"userId\"])\n",
    "churningusers  = churningusers.join(first_interaction,[\"userId\"])\n",
    "churningusers  = churningusers.join(last_interaction,[\"userId\"])\n",
    "churningusers  = churningusers.join(mean_interaction,[\"userId\"])\n",
    "churningusers = churningusers.withColumn('lastedinteraction', churningusers['last_interaction']-churningusers['first_interaction'])\n",
    "churningusers = churningusers.withColumn('lastedinteraction', churningusers['lastedinteraction']/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7386754b6d4bc9abc75071a72d58e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- churn: long (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- artistdiversity: long (nullable = false)\n",
      " |-- visitfrequency: long (nullable = false)\n",
      " |-- songcount: long (nullable = false)\n",
      " |-- sumoflength: double (nullable = true)\n",
      " |-- first_interaction: long (nullable = true)\n",
      " |-- last_interaction: long (nullable = true)\n",
      " |-- mean_interaction: double (nullable = true)\n",
      " |-- lastedinteraction: double (nullable = true)\n",
      "\n",
      "[Row(userId=0, churn=0, gender=0, level=0, artistdiversity=0, visitfrequency=0, songcount=0, sumoflength=17, first_interaction=0, last_interaction=0, mean_interaction=0, lastedinteraction=0)]"
     ]
    }
   ],
   "source": [
    "\n",
    "churningusers.printSchema()\n",
    "churningusers.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in churningusers.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a486f5b315340ea9655cdffe4cd5255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount', 'sumoflength', 'first_interaction', 'last_interaction', 'mean_interaction', 'lastedinteraction']"
     ]
    }
   ],
   "source": [
    "\n",
    "churningusers= churningusers.drop(\"userId\")\n",
    "getallfeatures = [mycol for mycol in churningusers.columns if mycol!='churn']\n",
    "print(getallfeatures)\n",
    "getallfeaturesall = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount', 'sumoflength', 'first_interaction', 'last_interaction', 'mean_interaction', 'lastedinteraction']\n",
    "\n",
    "getallfeatures1 = ['gender', 'level']\n",
    "getallfeatures2 = ['gender', 'level', 'artistdiversity' ]\n",
    "getallfeatures3 = ['gender', 'level', 'artistdiversity', 'visitfrequency']\n",
    "getallfeatures4 = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount']\n",
    "getallfeatures5 = ['gender', 'level', 'artistdiversity', 'visitfrequency', 'songcount', 'sumoflength']\n",
    "getallfeatures = getallfeatures2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccbd216326245c0b46fb47eb75c9fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- churn: long (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- artistdiversity: long (nullable = false)\n",
      " |-- visitfrequency: long (nullable = false)\n",
      " |-- songcount: long (nullable = false)\n",
      " |-- sumoflength: double (nullable = true)\n",
      " |-- first_interaction: long (nullable = true)\n",
      " |-- last_interaction: long (nullable = true)\n",
      " |-- mean_interaction: double (nullable = true)\n",
      " |-- lastedinteraction: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "#here, userid is no longer needed, as the model is lter on served by user data, without knowlegde of origin\n",
    "\n",
    "churningusers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9c63bee5ac4d96b35dce5080922798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train, test = churningusers.randomSplit([0.7, 0.3], seed=42)\n",
    "train = train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8c30f9b3234cd882cd27675a40e646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def modelbuilder(myclassifier, myparam, mynumfold):\n",
    "    \"\"\"\n",
    "    Try to setup a pipeline with given Clasifier\n",
    "    Input:\n",
    "    myclassifier - one type of vailable classifier classes\n",
    "    param - built param grid for  model opt search\n",
    "    Output :\n",
    "    mymodel - ML pipeline model as transformer...\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=getallfeatures, outputCol=\"features\")\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scfeatures\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, myclassifier])\n",
    "    mymodel = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=myparam,\n",
    "        evaluator = \\\n",
    "          MulticlassClassificationEvaluator( \\\n",
    "          labelCol='churn', metricName='f1'),\n",
    "        numFolds=mynumfold,\n",
    "    )\n",
    "    return mymodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf9b31236eb43729594bf8508ad7f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b89d339f9c40f79586a1a8b3744b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rndf = RandomForestClassifier(featuresCol=\"scfeatures\", labelCol=\"churn\")\n",
    "rndf_param = ParamGridBuilder().build()\n",
    "rndf_model = modelbuilder(rndf, rndf_param,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc672d9d638c471684bbf5aa32f231c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 using Random Forest: 0.6651134017337476"
     ]
    }
   ],
   "source": [
    "rndf_fitted_model = rndf_model.fit(train)\n",
    "rndf_pred = rndf_fitted_model.transform(test)\n",
    "\n",
    "rndfscore = evaluator.evaluate(rndf_pred, {evaluator.metricName: \"f1\"})\n",
    "print(\"f1 using Random Forest: {}\".format(rndfscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
